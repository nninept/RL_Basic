{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import wandb\n",
    "import torchvision.transforms as T\n",
    "# wandb.init(project=\"RL-Lec-Project\", entity=\"nninept\")\n",
    "print(torch.cuda.is_available())\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen = 5000)\n",
    "        self.minibatch_size = 64\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        mini_batch = random.sample(self.buffer, self.minibatch_size)\n",
    "        #mini_batch.append(self.buffer[-1])\n",
    "        s_lst, a_lst, r_lst, terminal, s_prime_lst = map(torch.FloatTensor, zip(*mini_batch))\n",
    "        return s_lst.to(device), np.array(a_lst), r_lst.to(device), terminal.to(device), s_prime_lst.to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(l):\n",
    "    if type(l) == nn.Linear:\n",
    "        nn.init.xavier_normal_(l.weight, gain=0.25)\n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128).to(device)\n",
    "        self.fc2 = nn.Linear(128, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, 2).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.to(device)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():    \n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.networkA = Net()\n",
    "        self.networkB = Net()\n",
    "        self.targetNetA = Net()\n",
    "        self.targetNetB = Net()\n",
    "        self.targetNetA.load_state_dict(self.networkA.state_dict())\n",
    "        self.targetNetB.load_state_dict(self.networkB.state_dict())\n",
    "        self.optimizerA = optim.RMSprop(self.networkA.parameters(), lr = learning_rate)\n",
    "        self.optimizerB = optim.RMSprop(self.networkB.parameters(), lr = learning_rate)\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.num_replay = 10\n",
    "        self.last_state = None\n",
    "        self.discount = 0.98\n",
    "        self.updateTerm = 100\n",
    "        self.epsilon = 0.1\n",
    "        self.tau = 0.1\n",
    "\n",
    "        self.maxStep = 0\n",
    "        self.updateA_count = 0\n",
    "        self.updateB_count = 0\n",
    "        \n",
    "    def sample_action(self, state, mode=\"train\"):\n",
    "        pi = (self.targetNetA(torch.from_numpy(state).float().to(device)) + self.targetNetB(torch.from_numpy(state).float().to(device)))/2\n",
    "        if mode == \"train\":\n",
    "          if np.random.random() < self.epsilon:\n",
    "              action = np.random.randint(2)\n",
    "              return action\n",
    "          else : \n",
    "              action = torch.argmax(pi).item()\n",
    "              return action\n",
    "        else:\n",
    "            action = torch.argmax(pi).item()\n",
    "            return action\n",
    "        \n",
    "    def train(self, epi):\n",
    "        totalStep = 0\n",
    "        self.last_state = self.env.reset()\n",
    "        while True:\n",
    "            # self.env.render()\n",
    "            totalStep += 1\n",
    "\n",
    "            action = self.sample_action(self.last_state)\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            # wandb.log({\"reward\": reward})\n",
    "            self.replay_buffer.append(self.last_state, action, reward, done, state) \n",
    "            if self.replay_buffer.size() >= self.replay_buffer.minibatch_size:\n",
    "                for _ in range(self.num_replay):\n",
    "                    if(np.random.random() < 0.5):\n",
    "                        self.optimize_network(self.networkA, self.targetNetB, \"A\")\n",
    "                        self.updateA_count += 1\n",
    "                    else:\n",
    "                        self.optimize_network(self.networkB, self.targetNetA, \"B\")\n",
    "                        self.updateB_count += 1\n",
    "\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            self.last_state = state\n",
    "\n",
    "        if(epi%self.updateTerm == 0):\n",
    "            # self.soft_update(self.networkA, self.targetNetA, self.tau)\n",
    "            # self.soft_update(self.networkB, self.targetNetB, self.tau)\n",
    "            self.targetNetA.load_state_dict(self.networkA.state_dict())\n",
    "            self.targetNetB.load_state_dict(self.networkB.state_dict())\n",
    "            print(f'#episode : {epi} \\t avg_step : {totalStep/self.updateTerm} \\t Update_A_ratio : {self.updateA_count / (self.updateA_count + self.updateB_count)} \\t Update_B_ratio : {self.updateB_count / (self.updateA_count + self.updateB_count)}')\n",
    "            totalStep = 0\n",
    "            self.updateA_count = 0  \n",
    "            self.updateB_count = 0\n",
    "            \n",
    "        if(self.maxStep < totalStep):\n",
    "            torch.save({\n",
    "                \"targetA\" : self.targetNetA.state_dict(),\n",
    "                \"targetB\" : self.targetNetA.state_dict()}, './checkpoint/DDQN_best.pt')\n",
    "            self.maxStep = totalStep\n",
    "            print(\"Save Best\")\n",
    "            \n",
    "    def optimize_network(self, network, targetNet, networkType):\n",
    "        states, actions, rewards, terminals, next_states = self.replay_buffer.sample()\n",
    "        \n",
    "        v_next_vec = torch.max(targetNet(next_states), dim = -1)[0]*(1-terminals)\n",
    "        target_vec = rewards + self.discount*v_next_vec\n",
    "        \n",
    "        q_mat = network(states)\n",
    "        batch_indices = np.arange(self.replay_buffer.minibatch_size)\n",
    "        q_vec = q_mat[batch_indices,actions]\n",
    "        \n",
    "        loss = F.mse_loss(q_vec,target_vec.detach())\n",
    "\n",
    "        if(networkType==\"A\"):\n",
    "            self.optimizerA.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizerA.step()\n",
    "        else:\n",
    "            self.optimizerB.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizerB.step()\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.config = {\n",
    "#   \"learning_rate\": 0.001,\n",
    "#   \"epochs\": 5000,\n",
    "#   \"batch_size\": 64\n",
    "# }\n",
    "model = Agent(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Best\n",
      "Save Best\n",
      "#episode : 100 \t avg_step : 0.12 \t Update_A_ratio : 0.4990639625585023 \t Update_B_ratio : 0.5009360374414976\n",
      "#episode : 200 \t avg_step : 0.08 \t Update_A_ratio : 0.4975296442687747 \t Update_B_ratio : 0.5024703557312253\n",
      "#episode : 300 \t avg_step : 0.12 \t Update_A_ratio : 0.4908839779005525 \t Update_B_ratio : 0.5091160220994475\n",
      "Save Best\n",
      "Save Best\n",
      "Save Best\n",
      "#episode : 400 \t avg_step : 0.09 \t Update_A_ratio : 0.49575551782682514 \t Update_B_ratio : 0.5042444821731749\n",
      "Save Best\n",
      "#episode : 500 \t avg_step : 0.11 \t Update_A_ratio : 0.5052316890881914 \t Update_B_ratio : 0.4947683109118087\n",
      "#episode : 600 \t avg_step : 0.12 \t Update_A_ratio : 0.5019966722129784 \t Update_B_ratio : 0.4980033277870216\n",
      "#episode : 700 \t avg_step : 0.11 \t Update_A_ratio : 0.49951026119402986 \t Update_B_ratio : 0.5004897388059701\n",
      "Save Best\n",
      "Save Best\n",
      "Save Best\n",
      "Save Best\n",
      "#episode : 800 \t avg_step : 0.94 \t Update_A_ratio : 0.5008080808080808 \t Update_B_ratio : 0.4991919191919192\n",
      "Save Best\n",
      "Save Best\n",
      "Save Best\n",
      "Save Best\n",
      "Save Best\n",
      "#episode : 900 \t avg_step : 2.74 \t Update_A_ratio : 0.5007350873669412 \t Update_B_ratio : 0.49926491263305883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/profjung/minsung/RL_Basic/Double DQN.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=0'>1</a>\u001b[0m pre \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epi \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5000\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(epi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=3'>4</a>\u001b[0m post \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(post\u001b[39m-\u001b[39mpre)\n",
      "\u001b[1;32m/home/profjung/minsung/RL_Basic/Double DQN.ipynb Cell 6\u001b[0m in \u001b[0;36mAgent.train\u001b[0;34m(self, epi)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=51'>52</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdateA_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=52'>53</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=53'>54</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize_network(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetworkB, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargetNetA, \u001b[39m\"\u001b[39;49m\u001b[39mB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=54'>55</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdateB_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=57'>58</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32m/home/profjung/minsung/RL_Basic/Double DQN.ipynb Cell 6\u001b[0m in \u001b[0;36mAgent.optimize_network\u001b[0;34m(self, network, targetNet, networkType)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=95'>96</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizerB\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=96'>97</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/profjung/minsung/RL_Basic/Double%20DQN.ipynb#ch0000005?line=97'>98</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizerB\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.9/site-packages/torch/optim/rmsprop.py:135\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m             grad_avgs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mgrad_avg\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    132\u001b[0m         state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 135\u001b[0m     F\u001b[39m.\u001b[39;49mrmsprop(params_with_grad,\n\u001b[1;32m    136\u001b[0m               grads,\n\u001b[1;32m    137\u001b[0m               square_avgs,\n\u001b[1;32m    138\u001b[0m               grad_avgs,\n\u001b[1;32m    139\u001b[0m               momentum_buffer_list,\n\u001b[1;32m    140\u001b[0m               lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    141\u001b[0m               alpha\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39malpha\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    142\u001b[0m               eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    143\u001b[0m               weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    144\u001b[0m               momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    145\u001b[0m               centered\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcentered\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.9/site-packages/torch/optim/_functional.py:252\u001b[0m, in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    250\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m--> 252\u001b[0m square_avg\u001b[39m.\u001b[39;49mmul_(alpha)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha)\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m centered:\n\u001b[1;32m    255\u001b[0m     grad_avg \u001b[39m=\u001b[39m grad_avgs[i]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pre = time.time()\n",
    "for epi in range(5000):\n",
    "    model.train(epi+1)\n",
    "post = time.time()\n",
    "print(post-pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testModel = Agent()\n",
    "\n",
    "env = gym.make('CartPole-v1') \n",
    "state = env.reset()\n",
    "i = 0\n",
    "\n",
    "checkpoint = torch.load(\"./checkpoint/DDQN_best.pt\")\n",
    "testModel.targetNetA.load_state_dict(checkpoint['targetA'])\n",
    "testModel.targetNetB.load_state_dict(checkpoint['targetB'])\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = testModel.sample_action(state,\"test\")\n",
    "    state, reward, done, _= env.step(action)\n",
    "    i+=1\n",
    "    if(done):\n",
    "        env.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c68e55b3957e083b8d9eb60684f58d9339eef94b09a512d325c3f26c66f2f15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
