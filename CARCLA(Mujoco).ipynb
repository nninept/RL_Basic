{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/profjung/minsung/RL_Basic/wandb/run-20220621_180318-3xzfldi8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nninept/CARCLA/runs/3xzfldi8\" target=\"_blank\">rose-elevator-20</a></strong> to <a href=\"https://wandb.ai/nninept/CARCLA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import wandb\n",
    "wandb.init(project=\"CARCLA\", entity=\"nninept\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen = 5000)\n",
    "        self.minibatch_size = 32\n",
    "\n",
    "    def append(self, state, action, reward, next_state, terminal):\n",
    "        self.buffer.append([state, action, reward, next_state, terminal])\n",
    "\n",
    "    def sample(self):\n",
    "        mini_batch = random.sample(self.buffer, self.minibatch_size)\n",
    "        s_lst, action, r_lst, s_prime_lst, done_mask_lst = map(list, zip(*mini_batch))\n",
    "        return torch.FloatTensor(s_lst).to(device), torch.FloatTensor(action).to(device), torch.FloatTensor(r_lst).to(device), \\\n",
    "                torch.FloatTensor(s_prime_lst).to(device), torch.FloatTensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self,action_space, observation_space, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionNetwork = nn.Sequential(\n",
    "            nn.Linear(observation_space[0], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,action_space[0]),\n",
    "            nn.Tanh()\n",
    "        ).to(device)\n",
    "        self.action_range = max_action[0]\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actionNetwork(state) * self.action_range\n",
    "\n",
    "    def select_action(self, state):\n",
    "        action = self.forward(state)\n",
    "        policy = torch.normal(action.detach(), 0.1)\n",
    "        policy = torch.clamp(policy, max=self.action_range, min=self.action_range*(-1))\n",
    "        return policy\n",
    "\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, observation_space):\n",
    "        super(Critic, self).__init__()\n",
    "        self.valueNetwork = nn.Sequential(\n",
    "            nn.Linear(observation_space[0], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.valueNetwork(state)\n",
    "\n",
    "\n",
    "\n",
    "class CARCLA():\n",
    "    def __init__(self, env):\n",
    "        super(CARCLA, self).__init__()\n",
    "        self.envName = env\n",
    "        self.env = gym.make(env)\n",
    "        self.actor = Actor(self.env.action_space.shape, self.env.observation_space.shape, self.env.action_space.high)\n",
    "        self.actor_target = Actor(self.env.action_space.shape, self.env.observation_space.shape, self.env.action_space.high)\n",
    "        self.critic = Critic(self.env.observation_space.shape)\n",
    "        self.critic_target = Critic(self.env.observation_space.shape)\n",
    "        self.actionOptimizer = optim.RMSprop(self.actor.parameters(), lr = 0.001)\n",
    "        self.valueOptimizer = optim.RMSprop(self.critic.parameters(), lr = 0.001)\n",
    "        self.criticLoss = nn.MSELoss()\n",
    "        self.actionLoss = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        wandb.watch(self.actor, self.actionLoss, log=\"all\", log_freq=10)\n",
    "        wandb.watch(self.critic, self.criticLoss, log=\"all\", log_freq=10)\n",
    "        self.tau = 0.001\n",
    "        self.discount = 0.9\n",
    "        self.num_replay = 15\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.maxStep = 0\n",
    "        self.maxReward = None\n",
    "        print(\"action space : \",self.env.action_space.shape)\n",
    "        print(\"action range : \",self.env.action_space.low, self.env.action_space.high)\n",
    "        \n",
    "    def train(self, epi):\n",
    "        self.last_state = self.env.reset()\n",
    "        totalReward = 0\n",
    "        count = 0\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            # self.env.render()\n",
    "            action = self.actor.select_action(torch.FloatTensor(self.last_state).to(device))\n",
    "            state, reward, done, _= self.env.step(action.detach().cpu().numpy())\n",
    "            count += 1\n",
    "            totalReward += reward\n",
    "            \n",
    "\n",
    "            self.replay_buffer.append(self.last_state, action.detach().cpu().numpy(), reward, state, done)\n",
    "            if self.replay_buffer.size()>self.replay_buffer.minibatch_size:\n",
    "                for _ in range(self.num_replay):\n",
    "                    self.optimize_network()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            self.last_state = state\n",
    "        wandb.log({\"Accumulated Reward\": totalReward, \"Avg Reward\":totalReward/count, \"Step\" : count})\n",
    "        if((epi+1)%10 == 0):\n",
    "            print(f'Epi : {epi} \\t Avg reward : {totalReward/count} \\t Step : {count}')\n",
    "            \n",
    "        # if(self.maxStep < count):\n",
    "        #     torch.save(model.actor.state_dict(), './checkpoint/CARCLA_best.pt')\n",
    "        #     self.maxStep = count\n",
    "        #     print(\"Save Best\")\n",
    "        \n",
    "        if(self.maxReward == None or self.maxReward < totalReward):\n",
    "            torch.save(model.actor.state_dict(), f'./checkpoint/CARCLA_best_{self.envName}.pt')\n",
    "            self.maxReward = totalReward\n",
    "            print(\"Save Best Reward : \",totalReward)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def optimize_network(self):\n",
    "        states, actions, rewards, next_states, terminals = self.replay_buffer.sample()\n",
    "        q_next_mat = self.critic_target(next_states).view(-1)\n",
    "        targetQ = rewards + q_next_mat*(1-terminals)*self.discount\n",
    "    \n",
    "        self.valueOptimizer.zero_grad()\n",
    "        q_mat = self.critic(states).view(-1)\n",
    "        valueLoss = self.criticLoss(q_mat,targetQ)\n",
    "        valueLoss.backward()\n",
    "        self.valueOptimizer.step()\n",
    "        \n",
    "        \n",
    "        policyUpdateIdx = targetQ - q_mat > 0\n",
    "        policy_evaluation = self.criticLoss(actions[policyUpdateIdx] ,self.actor.forward(states[policyUpdateIdx]))\n",
    "        self.actionOptimizer.zero_grad()\n",
    "        policy_evaluation.backward()\n",
    "        self.actionOptimizer.step()\n",
    "        \n",
    "        self.soft_update(self.critic, self.critic_target, self.tau)\n",
    "        self.soft_update(self.actor, self.actor_target, self.tau)\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/profjung/anaconda3/envs/mujoco/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:70: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
      "  \"Agent's minimum action space value is -infinity. This is probably too low.\"\n",
      "/home/profjung/anaconda3/envs/mujoco/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:74: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
      "  \"Agent's maximum action space value is infinity. This is probably too high\"\n",
      "/home/profjung/anaconda3/envs/mujoco/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space :  (17,)\n",
      "action range :  [-0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4\n",
      " -0.4 -0.4 -0.4] [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "Save Best Reward :  236.8653802931834\n",
      "Save Best Reward :  261.3112659634742\n",
      "Epi : 9 \t Avg reward : 4.947486029516779 \t Step : 34\n",
      "Epi : 19 \t Avg reward : 5.232825932228449 \t Step : 26\n",
      "Epi : 29 \t Avg reward : 5.206878534835963 \t Step : 41\n",
      "Epi : 39 \t Avg reward : 5.044311569419822 \t Step : 37\n",
      "Epi : 49 \t Avg reward : 5.114286039084454 \t Step : 37\n",
      "Epi : 59 \t Avg reward : 5.206456132884164 \t Step : 35\n",
      "Save Best Reward :  280.70495302911564\n",
      "Epi : 69 \t Avg reward : 5.090491334504302 \t Step : 31\n",
      "Epi : 79 \t Avg reward : 5.143215930056289 \t Step : 29\n",
      "Epi : 89 \t Avg reward : 5.277297423567667 \t Step : 27\n",
      "Epi : 99 \t Avg reward : 5.1200449251898545 \t Step : 40\n",
      "Epi : 109 \t Avg reward : 5.181283760892184 \t Step : 28\n",
      "Epi : 119 \t Avg reward : 4.989342019432581 \t Step : 34\n",
      "Epi : 129 \t Avg reward : 5.327933826810138 \t Step : 33\n",
      "Epi : 139 \t Avg reward : 5.230935517475803 \t Step : 33\n",
      "Epi : 149 \t Avg reward : 5.184202261311572 \t Step : 29\n",
      "Epi : 159 \t Avg reward : 5.207800025122089 \t Step : 32\n",
      "Epi : 169 \t Avg reward : 5.076348459475293 \t Step : 28\n",
      "Epi : 179 \t Avg reward : 5.037100126556894 \t Step : 38\n",
      "Epi : 189 \t Avg reward : 5.245706611465212 \t Step : 38\n",
      "Epi : 199 \t Avg reward : 5.32441234407729 \t Step : 28\n",
      "Epi : 209 \t Avg reward : 5.059406446541971 \t Step : 41\n",
      "Epi : 219 \t Avg reward : 5.21322782950798 \t Step : 37\n",
      "Epi : 229 \t Avg reward : 4.972929363584623 \t Step : 36\n",
      "Epi : 239 \t Avg reward : 5.161606396194157 \t Step : 28\n",
      "Epi : 249 \t Avg reward : 5.359285615951841 \t Step : 41\n",
      "Epi : 259 \t Avg reward : 5.25556400608692 \t Step : 32\n",
      "Epi : 269 \t Avg reward : 5.106581473364893 \t Step : 39\n",
      "Epi : 279 \t Avg reward : 5.181229960442466 \t Step : 29\n",
      "Epi : 289 \t Avg reward : 5.2529905303257305 \t Step : 28\n",
      "Epi : 299 \t Avg reward : 5.357456008978835 \t Step : 29\n",
      "Epi : 309 \t Avg reward : 5.202986067244718 \t Step : 35\n",
      "Epi : 319 \t Avg reward : 5.198512237356402 \t Step : 23\n",
      "Epi : 329 \t Avg reward : 5.204901804833123 \t Step : 26\n",
      "Epi : 339 \t Avg reward : 5.011789480897996 \t Step : 27\n",
      "Epi : 349 \t Avg reward : 5.209617039799086 \t Step : 50\n",
      "Epi : 359 \t Avg reward : 5.021150183745297 \t Step : 40\n",
      "Epi : 369 \t Avg reward : 5.172373192231389 \t Step : 35\n",
      "Epi : 379 \t Avg reward : 5.087793340019184 \t Step : 34\n",
      "Epi : 389 \t Avg reward : 5.352477940777858 \t Step : 38\n",
      "Epi : 399 \t Avg reward : 5.1395831355776815 \t Step : 33\n",
      "Epi : 409 \t Avg reward : 5.263970207489189 \t Step : 24\n",
      "Epi : 419 \t Avg reward : 5.245639532466886 \t Step : 24\n",
      "Epi : 429 \t Avg reward : 5.277427481066123 \t Step : 29\n",
      "Epi : 439 \t Avg reward : 5.295309173270069 \t Step : 31\n",
      "Epi : 449 \t Avg reward : 5.278946906248273 \t Step : 31\n",
      "Epi : 459 \t Avg reward : 5.273711223016346 \t Step : 31\n",
      "Epi : 469 \t Avg reward : 5.302526006649274 \t Step : 27\n",
      "Epi : 479 \t Avg reward : 5.296101465759776 \t Step : 28\n",
      "Epi : 489 \t Avg reward : 5.2420402130502985 \t Step : 24\n",
      "Epi : 499 \t Avg reward : 5.203896113898314 \t Step : 28\n",
      "Epi : 509 \t Avg reward : 5.296375652995965 \t Step : 33\n",
      "Epi : 519 \t Avg reward : 5.259595422308643 \t Step : 34\n",
      "Epi : 529 \t Avg reward : 5.224990169270087 \t Step : 37\n",
      "Epi : 539 \t Avg reward : 5.403787519459305 \t Step : 30\n",
      "Epi : 549 \t Avg reward : 5.230624417076113 \t Step : 26\n",
      "Epi : 559 \t Avg reward : 5.267540750087417 \t Step : 31\n",
      "Epi : 569 \t Avg reward : 5.1776139082102 \t Step : 27\n",
      "Epi : 579 \t Avg reward : 5.22516576913022 \t Step : 24\n",
      "Epi : 589 \t Avg reward : 5.141658046769269 \t Step : 32\n",
      "Epi : 599 \t Avg reward : 5.231308167410995 \t Step : 30\n",
      "Epi : 609 \t Avg reward : 5.27073694713666 \t Step : 26\n",
      "Epi : 619 \t Avg reward : 5.221280527838362 \t Step : 36\n",
      "Epi : 629 \t Avg reward : 5.145335365247638 \t Step : 29\n",
      "Epi : 639 \t Avg reward : 5.09006683728837 \t Step : 30\n",
      "Epi : 649 \t Avg reward : 5.317759813217492 \t Step : 33\n",
      "Epi : 659 \t Avg reward : 4.98697670462007 \t Step : 38\n",
      "Epi : 669 \t Avg reward : 4.817536049374144 \t Step : 33\n",
      "Epi : 679 \t Avg reward : 5.156199912702634 \t Step : 35\n",
      "Epi : 689 \t Avg reward : 5.265391574596463 \t Step : 27\n",
      "Epi : 699 \t Avg reward : 5.231676103751393 \t Step : 30\n",
      "Epi : 709 \t Avg reward : 5.112800983142459 \t Step : 29\n",
      "Epi : 719 \t Avg reward : 5.217491138561175 \t Step : 31\n",
      "Epi : 729 \t Avg reward : 5.174041290383352 \t Step : 28\n",
      "Epi : 739 \t Avg reward : 5.132550990752085 \t Step : 24\n",
      "Epi : 749 \t Avg reward : 5.212411857721444 \t Step : 30\n",
      "Epi : 759 \t Avg reward : 5.182694077774225 \t Step : 30\n",
      "Epi : 769 \t Avg reward : 5.236896604722808 \t Step : 26\n",
      "Epi : 779 \t Avg reward : 4.9751215401236895 \t Step : 30\n",
      "Epi : 789 \t Avg reward : 5.416992046109787 \t Step : 32\n",
      "Epi : 799 \t Avg reward : 5.151367767065908 \t Step : 30\n",
      "Epi : 809 \t Avg reward : 5.082947414419405 \t Step : 19\n",
      "Epi : 819 \t Avg reward : 5.164768291544683 \t Step : 22\n",
      "Epi : 829 \t Avg reward : 5.044505953111817 \t Step : 42\n",
      "Epi : 839 \t Avg reward : 5.22640841389335 \t Step : 36\n",
      "Epi : 849 \t Avg reward : 5.230219912048284 \t Step : 22\n",
      "Epi : 859 \t Avg reward : 5.077210338221792 \t Step : 24\n",
      "Epi : 869 \t Avg reward : 5.062048901390604 \t Step : 34\n",
      "Epi : 879 \t Avg reward : 5.1052653018476395 \t Step : 32\n",
      "Epi : 889 \t Avg reward : 5.1648328762096 \t Step : 43\n",
      "Epi : 899 \t Avg reward : 5.150586276794844 \t Step : 21\n",
      "Epi : 909 \t Avg reward : 5.124675370022751 \t Step : 31\n",
      "Epi : 919 \t Avg reward : 5.141609389286984 \t Step : 21\n",
      "Epi : 929 \t Avg reward : 5.2510100896083545 \t Step : 31\n",
      "Epi : 939 \t Avg reward : 5.264358755262628 \t Step : 40\n",
      "Epi : 949 \t Avg reward : 5.114851043719349 \t Step : 26\n",
      "Epi : 959 \t Avg reward : 5.201240920270637 \t Step : 37\n",
      "Epi : 969 \t Avg reward : 5.1399170714308715 \t Step : 27\n",
      "Epi : 979 \t Avg reward : 5.224402704669127 \t Step : 36\n",
      "Epi : 989 \t Avg reward : 5.087174354602438 \t Step : 33\n",
      "Epi : 999 \t Avg reward : 5.0712005410802785 \t Step : 40\n"
     ]
    }
   ],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 1000,\n",
    "  \"batch_size\": 32,\n",
    "  \"tau\" : 0.001\n",
    "}\n",
    "env = \"Humanoid-v4\"\n",
    "model = CARCLA(env)\n",
    "\n",
    "for epi in range(1000):\n",
    "    model.train(epi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/profjung/anaconda3/envs/mujoco/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# newmodel = CARCLA(env)\n",
    "# newmodel.actor.load_state_dict(torch.load('./checkpoint/CARCLA_best.pt'))\n",
    "env = gym.make(env) \n",
    "state = env.reset()\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = model.actor.select_action(torch.FloatTensor(state).to(device))\n",
    "    state, reward, done, _= env.step(action.detach().cpu().numpy())\n",
    "    i+=1\n",
    "    if(done):\n",
    "        env.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accumulated Reward</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▄▃▃▄▃▂▄▃▂▂▃▄▅▃▄▅█▄▄▁▄▇▅▂</td></tr><tr><td>Avg Reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Step</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▄▃▃▄▃▂▄▃▂▂▃▄▅▃▄▅█▄▄▁▄▇▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accumulated Reward</td><td>641.0</td></tr><tr><td>Avg Reward</td><td>1.0</td></tr><tr><td>Step</td><td>641</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">winter-planet-19</strong>: <a href=\"https://wandb.ai/nninept/CARCLA/runs/39je0m67\" target=\"_blank\">https://wandb.ai/nninept/CARCLA/runs/39je0m67</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220621_163431-39je0m67/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c68e55b3957e083b8d9eb60684f58d9339eef94b09a512d325c3f26c66f2f15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
