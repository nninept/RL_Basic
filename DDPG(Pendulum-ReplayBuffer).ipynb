{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen = 5000)\n",
    "        self.minibatch_size = 63\n",
    "\n",
    "    def append(self, state, action, reward, next_state, terminal):\n",
    "        self.buffer.append([state, action, reward, next_state, terminal])\n",
    "\n",
    "    def sample(self):\n",
    "        mini_batch = random.sample(self.buffer, self.minibatch_size)\n",
    "        mini_batch.append(self.buffer[-1])\n",
    "        s_lst, action, r_lst, s_prime_lst, done_mask_lst = map(list, zip(*mini_batch))\n",
    "        return torch.FloatTensor(s_lst).to(device), torch.FloatTensor(action).to(device), torch.FloatTensor(r_lst).to(device), \\\n",
    "                torch.FloatTensor(s_prime_lst).to(device), torch.FloatTensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionNetwork = nn.Sequential(\n",
    "            nn.Linear(3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actionNetwork(state)\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.valueNetwork = nn.Sequential(\n",
    "            nn.Linear(3+1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        state = state.view(-1,state.shape[-1])\n",
    "        action = action.view(-1,action.shape[-1])\n",
    "        return self.valueNetwork(torch.cat([state, action], dim = -1))\n",
    "\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(self):\n",
    "        super(DDPG, self).__init__()\n",
    "        self.actor = Actor()\n",
    "        self.actor_target = Actor()\n",
    "        self.critic = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.actionOptimizer = optim.Adam(self.actor.parameters(), lr = 0.001)\n",
    "        self.valueOptimizer = optim.Adam(self.critic.parameters(), lr = 0.001)\n",
    "        self.criticLoss = nn.MSELoss()\n",
    "        self.env = gym.make('Pendulum-v1')\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.tau = 0.001\n",
    "        self.gamma = 0.9\n",
    "        self.num_replay = 15\n",
    "        self.reward = 0\n",
    "        self.count = 0\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.flag = False\n",
    "        print(\"action space : \",self.env.action_space.shape)\n",
    "        \n",
    "    def train(self, epi):\n",
    "        self.last_state = self.env.reset()\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.actor(torch.FloatTensor(self.last_state).to(device))\n",
    "            state, reward, done, _= self.env.step(action.detach().cpu().numpy())\n",
    "            self.count += 1\n",
    "            self.reward += reward\n",
    "            \n",
    "            self.replay_buffer.append(self.last_state, action.detach().cpu().numpy(), reward, state, done)\n",
    "            if self.replay_buffer.size()>self.replay_buffer.minibatch_size:\n",
    "                for _ in range(self.num_replay):\n",
    "                    self.optimize_network()\n",
    "\n",
    "            if done:\n",
    "                print(f'Epi : {epi}   Avg reward : {self.reward/self.count}')\n",
    "                break\n",
    "            self.last_state = state\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def optimize_network(self):\n",
    "        states, actions, rewards, next_states, terminals = self.replay_buffer.sample()\n",
    "        q_next_mat = self.critic_target(next_states, self.actor_target(next_states)).view(-1)\n",
    "        targetQ = rewards + q_next_mat*(1-terminals)*self.gamma\n",
    "    \n",
    "        self.valueOptimizer.zero_grad()\n",
    "        q_mat = self.critic(states, actions).view(-1)\n",
    "        valueLoss = self.criticLoss(q_mat,targetQ)\n",
    "        valueLoss.backward()\n",
    "        self.valueOptimizer.step()\n",
    "        \n",
    "        self.actionOptimizer.zero_grad()\n",
    "        q_mat = self.critic(states, self.actor(states)).view(-1)\n",
    "        actionLoss = (-q_mat.mean()).backward()\n",
    "        self.actionOptimizer.step()\n",
    "        \n",
    "        self.soft_update(self.critic, self.critic_target, self.tau)\n",
    "        self.soft_update(self.actor, self.actor_target, self.tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "Epi : 0   Avg reward : -5.406453780774277\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = DDPG()\n",
    "\n",
    "for epi in range(1):\n",
    "    model.train(epi)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epi : 0   Avg reward : -7.331660270690918\n",
    "Epi : 1   Avg reward : -6.766748428344727\n",
    "Epi : 2   Avg reward : -7.019373416900635\n",
    "Epi : 3   Avg reward : -7.0845561027526855\n",
    "Epi : 4   Avg reward : -7.081833362579346\n",
    "Epi : 5   Avg reward : -7.101948261260986\n",
    "Epi : 6   Avg reward : -6.921932220458984\n",
    "Epi : 7   Avg reward : -6.668462753295898\n",
    "Epi : 8   Avg reward : -6.543397903442383\n",
    "Epi : 9   Avg reward : -6.431257724761963\n",
    "Epi : 10   Avg reward : -6.147215843200684\n",
    "Epi : 11   Avg reward : -5.742638111114502\n",
    "Epi : 12   Avg reward : -5.726519584655762\n",
    "Epi : 13   Avg reward : -5.541481971740723\n",
    "Epi : 14   Avg reward : -5.26165246963501\n",
    "Epi : 15   Avg reward : -5.021037578582764\n",
    "Epi : 16   Avg reward : -4.803654193878174\n",
    "Epi : 17   Avg reward : -4.693840026855469\n",
    "Epi : 18   Avg reward : -4.8417816162109375\n",
    "Epi : 19   Avg reward : -4.664757251739502\n",
    "Epi : 20   Avg reward : -4.503854274749756\n",
    "Epi : 21   Avg reward : -4.567991256713867\n",
    "Epi : 22   Avg reward : -4.39901065826416\n",
    "Epi : 23   Avg reward : -4.245830059051514\n",
    "Epi : 24   Avg reward : -4.188897132873535\n",
    "Epi : 25   Avg reward : -4.07916259765625\n",
    "Epi : 26   Avg reward : -4.208540916442871\n",
    "Epi : 27   Avg reward : -4.083531379699707\n",
    "Epi : 28   Avg reward : -3.966477632522583\n",
    "Epi : 29   Avg reward : -3.856781005859375\n",
    "Epi : 30   Avg reward : -3.7535760402679443\n",
    "Epi : 31   Avg reward : -3.6942529678344727\n",
    "Epi : 32   Avg reward : -3.6025590896606445\n",
    "Epi : 33   Avg reward : -3.515573024749756\n",
    "Epi : 34   Avg reward : -3.4332034587860107\n",
    "Epi : 35   Avg reward : -3.374929904937744\n",
    "Epi : 36   Avg reward : -3.3011815547943115\n",
    "Epi : 37   Avg reward : -3.2485601902008057\n",
    "Epi : 38   Avg reward : -3.226588249206543\n",
    "Epi : 39   Avg reward : -3.162376642227173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'C:\\\\ProgramStudy\\\\RL_Drone\\\\AirSim\\\\PythonClient\\\\multirotor\\\\parameter.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c68e55b3957e083b8d9eb60684f58d9339eef94b09a512d325c3f26c66f2f15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
